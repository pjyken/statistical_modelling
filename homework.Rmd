---
title: "Statistical Modeling: Homework 2016"
author: "Richard Podkolinski"
date: "8 April 2016"
output: html_document
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(12345)
library(MASS)
library(faraway)
library(dplyr)
library(ggplot2)
library(GGally)
library(rethinking)
options(scipen = 999)
df = motorins
```

## Question 1

We first explore a dataset of third part motor insurance claims. The data is the aggregate of insurance claims given specific conditions, this is possible because at the time consumers were given identical risk arguments[1]. So effectively we have sums of accidents of similar insurance policies.

Before we get to modeling, we should take a look at our data. 

There are 8 variabes:

- **Kilometers**: Ordinal variable, amount of kilometers per year driven.
- **Zone**: Nominal variable defining geographical location.
- **Bonus**: Integer, no-claims bonus. Number of years plus one since last claim.
- **Make**: Nominal variable, defining car model, 9 is other.
- **Insured**: Real, number of insured in policy-years.
- **Claims**: Integer, number of claims.
- **Payment**: Integer, Total value of payments in Swedish Korona. 
- **perd**: Real, payment per claim and our response variable.

Even a cursory reading of these variables indicates that there is a going to be a multicollinarity problem in the data. The response variable is a ratio of two of the predictors (**pred** = **Payment** / **Claims**) thus the inclusion of both in any model is not advised. 


We then plot the density of the response variable. 

```{r, echo=FALSE}
df %>% ggplot(aes(x = perd)) + geom_density(color = "slateblue", size = 1) + theme_bw() + ggtitle("Density of Payment per Claim") + geom_text(aes(x= 30000, y = 0.00005, label = "Outliers")) + geom_segment(aes(x = 30000, y = 0.000045, xend = 31000, yend = 0.000015), arrow = arrow(length = unit(0.03, "npc")))
```

As it is stated in the question, we see a positive value, positively skewed distribution. We also see a small pocket of outliers in the tail of the distribution. A gamma distribution would seem appropriate in this context. We make use of the default link function, the inverse link $ g(\mu_i) = \frac{1}{\mu_i} $. 

We then select covariates. However, there is clearly two variables where there is multicolinarity, we will refrain from using automatic step function as it will crash `glm` due to a singular matrix error. We can, however, easily loop over the entire set of possible models and retrieve the AIC and BIC of each valid model while catching the fitting error. 

The following table contains the selected variables, the type of link function and their respective information criterion score. Only the best of each are displayed. It should be noted that the asterix in the Variables column implies the presence of all interaction effects between variables, as it does in R's formula inputs. Two main types of models are explored, main effects and full interaction. 



```{r}
safe_stepAIC = function(df, link="inverse", type="+") {
  covars = names(df)[-8]
  respn = "perd ~ "
  output = data.frame(Form = character(), AIC = numeric(), BIC = numeric())
  for(i in 6:1){
    print(i)
    lcovars = combn(covars, i, simplify = F)  
    for(i in 1:length(lcovars)){
      covara = lcovars[[i]]
      covarm = paste(covara, collapse = type)
      gform = as.formula(paste(respn, covarm))
      print(covarm)
      tryCatch({
        fit =  glm(gform, family = Gamma(link = link), data = df)
        fitaic = AIC(fit)
        fitbic = BIC(fit)
        inn_out = data.frame(Form = paste0(fit$formula, collapse = " "), AIC = fitaic, BIC = fitbic)
        output = rbind(output, inn_out)
      }, error = function(e){})
    }
  }
  return(output)
}
```




```{r, message=FALSE, warning=FALSE, include=FALSE}
# Recheck best models

# For Inverse Link selection
intr_model = perd ~ Kilometres * Zone * Make * Claims * Payment
main_model = perd ~ Make + Claims + Payment

fiti = glm(intr_model, family = Gamma(link = "inverse"), data = df)
fitm = glm(main_model, family = Gamma(link = "inverse"), data = df)

AIC(fiti)
BIC(fiti)

AIC(fitm)
BIC(fitm)


# For Log Selection
liti = glm(intr_model, family = Gamma(link = "log"), data = df)
litm = glm(main_model, family = Gamma(link = "log"), data = df)

AIC(liti)
BIC(liti)
AIC(litm)
BIC(litm)

# For Identity Selection
iiti = glm(intr_model, family = Gamma(link = "identity"), data = df)
iitm = glm(main_model, family = Gamma(link = "identity"), data = df)

AIC(iiti)
BIC(iiti)
AIC(iitm)
BIC(iitm)


# Best BIC Model
intr_bmod = perd ~ Bonus * Make * Insured * Claims * Payment
main_bmod = perd ~ Make + Claims + Payment

bitm = glm(main_bmod, family = Gamma(link = "inverse"), data = df)
AIC(bitm)
BIC(bitm)

biti = glm(intr_bmod, family = Gamma(link = "log"), data = df)
AIC(biti)
BIC(biti)
```

| row  | Formula                                        | Link     | AIC           | BIC           | Converged? |
|------|------------------------------------------------|----------|---------------|---------------|------------|
| 1    | `r paste(intr_model[c(-1,-2)], collapse = "")` | inverse  | `r AIC(fiti)` | `r BIC(fiti)` | Yes        |
| 2    | `r paste(main_model[c(-1,-2)], collapse = "")` | inverse  | `r AIC(fitm)` | `r BIC(fitm)` | Yes        |
| 3    | `r paste(intr_model[c(-1,-2)], collapse = "")` | log      | `r AIC(liti)` | `r BIC(liti)` | Yes        |
| 4    | `r paste(main_model[c(-1,-2)], collapse = "")` | log      | `r AIC(litm)` | `r BIC(litm)` | No         |
| 5    | `r paste(intr_model[c(-1,-2)], collapse = "")` | identity | `r AIC(iiti)` | `r BIC(iiti)` | No         |
| 6    | `r paste(main_model[c(-1,-2)], collapse = "")` | identity | `r AIC(litm)` | `r BIC(iitm)` | No         |
| 7    | `r paste(intr_bmod[c(-1,-2)], collapse = "")`  | inverse  | `r AIC(biti)` | **`r BIC(biti)`** | Yes        |
| 8    | `r paste(main_bmod[c(-1,-2)], collapse = "")`  | log      | `r AIC(bitm)` | `r BIC(bitm)` | Yes        |

**(a)**

Several things can be observed from the first six rows of the table. Surprisingly, models that included both **Claims** and **Payments** returned the lowest information criteria. Fortunately, multicollinearity does not affect measures based on information criteria[2]. However, it does inflate standard errors of parameter estimates, thus it is the use of the model that should determine if these variables are retained. We can see that if we select a model with AIC, we would select the model with the largest complexity, one with five variables and all the interactions between them. This reaffirms the general notion that AIC can overfit model selection. 

**(b)**

The use of a log or identity link in the Gamma regression often improves the fit (according to out-of-sample deviance judged by AIC/BIC) but often leads to models that have difficulty converging. Providing better starting values for the `glm` function may alieviate this issue.

**(c)**

Generally, BIC prefers simpler models, this directly the result of a more severe penalty for model complexity. Somewhat surprisingly, in this case (row 7), we find a complex model with five covariates and all interactions that has the lowest BIC of any model. 

**(d)**

```{r, message=FALSE, warning=FALSE, include=FALSE}
dfc = df %>% select(-perd)
pfit = glm(Claims ~ ., family = poisson, data = dfc)
summary(pfit)

qfit = glm(Claims ~ ., family = quasipoisson, data = dfc)
summary(qfit)

qfit_dev = round(sum(residuals(qfit, type = "pearson")^2)/df.residual(qfit), 2)
```

At first viewing the Poisson model appears to be quite appropriate for predicting **Claims**. However, deeper examination indicates that the assumption of conditional mean equalling conditional variance is not true. This is known as under/overdispersion. By fitting **Claims** with a QuasiPoisson model we are able to explicitly model the degree of overdispersion which is `r qfit_dev` in this case.




## Question 2

**(a)**
```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(SemiPar)
dft = data(ustemp)
dft = ustemp

afit = lm(min.temp ~ longitude + latitude, data = dft)
ifit = lm(min.temp ~ longitude + latitude + longitude:latitude, data = dft)

summary(afit)
summary(ifit)
AIC(afit)
AIC(ifit)
# plot(stdres(ifit))
# abline(h=c(-2.5,2.5),lty=2)
qqnorm(dft$min.temp)
qqline(dft$min.temp)

# library(nortest)
# lillie.test(dft$min.temp)
```



**(a)**

We fit two models:
$$ mintemp = \beta_0 + \beta_{lon} \text{longitude} + \beta_{lat} \text{latitute} + \varepsilon $$
$$ mintemp = \beta_0 + \beta_{lon} \text{longitude} + \beta_{lat} \text{latitute} + \beta_{lon_lat} \text{longitude latitutde}  + \varepsilon $$

The AIC of the first model is `r AIC(afit)` and for the second model `r AIC(ifit)`, we can see the second more complex model is preferred.

**(b)**

```{r}
pvalue = function(c_n, m = 1e5) {
  1 - exp((-sum((1 - pchisq((1:m) * c_n, 1:m))/(1:m))))
}

# order = 3
# oloc = poly(dft$longitude, dft$latitude)
# term_n = function(x) x * (x + 2)
# tot_terms = term_n(3)

library(SemiPar)
data(ustemp)
X = poly(ustemp$latitude, ustemp$longitude)
x1 = X[,1]
x2 = X[,2]
Y  = ustemp$min.temp

compare_models = function(null_formula, alt_formula, Cn = 4.18){
	
	null_model = lm(null_formula)
	null_logLik = logLik(null_model)

	altn_model = lm(alt_formula)
	altn_logLik = logLik(altn_model)
	
	Tn = 2 * (altn_logLik - null_logLik) / (attr(altn_logLik, "df") - attr(null_logLik, "df"))

	reject = Tn > Cn

	# return(reject)
	out = data.frame(null_model = paste(null_formula, collapse = ""), 
	                 altn_model = paste(alt_formula, collapse = ""), 
	                 Tn = Tn, Reject = reject)
	return(out)
}

model_list = function(order) {
	model_form = vector()
	k=1
	for(i in 1:order){
	  for(j in 0:(i-1)){
	    model_form[k] = paste("I(x1^",i," * ","x2^", j,")", sep="")
	    k=k+1
	    model_form[k] = paste("I(x1^",j," * ","x2^", i,")", sep="")
	    k=k+1
	  }
	  model_form[k] = paste("I(x1^",i," * ","x2^", i,")", sep="")
	  k=k+1
	}
	return(model_form)
}

model_form = model_list(3)

null_model = Y ~ I(x1) + I(x2)

compare_models(null_model, z)

sapply(3:length(model_form), function(x){
  compare_models(null_model, paste("Y ~ ", paste(model_form[1:x], collapse = "+")))
})


```





## Question 3

**(a)**

$$
\begin{align*}
&\text{We compute the bias of the } \hat{\beta}_R \text{ we begin with its stated definition:} \\
\hat{\beta}_R &= (X^t X + \lambda I_p)^{-1} X^t Y \\
&\text{We know that }I_p \text{ can be expanded through the identity } I = (X^t X)(X^t X)^{-1} \\
\hat{\beta}_R &= (X^t X + \lambda (X^t X)(X^t X)^{-1})^{-1} X^t Y \\
\hat{\beta}_R &= (I_p + \lambda (X^t X)^{-1})^{-1} (X^t X)^{-1} X^t Y \\
&\text{Further, we know the linear model matrix form is } Y = X \beta + \varepsilon \\
\hat{\beta}_R &= (I_p + \lambda (X^t X)^{-1})^{-1} (X^t X)^{-1} X^t (X \beta + \varepsilon )\\
\hat{\beta}_R &= (I_p + \lambda (X^t X)^{-1})^{-1} \beta + (I_p + \lambda (X^t X)^{-1})^{-1} (X^t X)^{-1} X^t \varepsilon \\
&\text{We can then take the expectation of the above} \\
E(\hat{\beta}_R) &= E[(I_p + \lambda (X^t X)^{-1})^{-1} \beta] + E[(I_p + \lambda (X^t X)^{-1})^{-1} (X^t X)^{-1} X^t \varepsilon] \\
&\text{The value of } E[\varepsilon] = 0 \\
E(\hat{\beta}_R) &= (I_p + \lambda (X^t X)^{-1})^{-1} E[\beta] \\
&\text{The least-squares property tells us that } E[\beta] = \beta \\
E(\hat{\beta}_R) &= (I_p + \lambda (X^t X)^{-1})^{-1} \beta \\
&\text{We can then determine the bias} \\
\text{Bias} &= E(\hat{\beta}_R) - \beta \\
\text{Bias} &= (I_p + \lambda (X^t X)^{-1})^{-1} \beta - \beta \\
\text{Bias} &= ((I_p + \lambda (X^t X)^{-1})^{-1})-I_p) \beta
\end{align*}
$$

**(b)**
$$
\begin{align*}
\text{Var}(\hat{\beta}_R) &= \text{Var}(A \hat{\beta}_{LS}) \\
\text{Var}(\hat{\beta}_R) &= A \text{Var}(\hat{\beta}_{LS}) A^t\\
\text{Var}(\hat{\beta}_R) &= \sigma^2 A (X^t X)^{-1} A^t \\
&\text{We can then expand } A = (I_p + \lambda(X^t X)^{-1})^{-1} \\
\text{Var}(\hat{\beta}_R) &= \sigma^2 (I_p + \lambda(X^t X)^{-1})^{-1} (X^t X)^{-1} ((I_p + \lambda(X^t X)^{-1})^{-1})^t \\
\end{align*}
$$


**(c)**
$$
\begin{align*}
\hat{Y}_R &= X \hat{\beta}_R \\
E[\hat{Y}_R] &= E(X \hat{\beta}_R) \\
E[\hat{Y}_R] &= X E(\hat{\beta}_R) \\
E[\hat{Y}_R] &= X (I_p + \lambda (X^t X)^{-1})^{-1} \beta \\
\\
\text{Bias}(\hat{Y}_R) &= E(\hat{Y}_R) - E(Y) \\
\text{Bias}(\hat{Y}_R) &= X (I_p + \lambda (X^t X)^{-1})^{-1} \beta - X \beta \\
\text{Bias}(\hat{Y}_R) &= X ((I_p + \lambda (X^t X)^{-1})^{-1} - I_p) \beta \\
\\
\text{Var}(\hat{Y}_R) &= \text{Var}(X \hat{\beta}_R) \\
\text{Var}(\hat{Y}_R) &= \text{Var}(X A \hat{\beta}_{LS}) \\
\text{Var}(\hat{Y}_R) &= (XA) \text{Var}(\hat{\beta}_{LS})(XA)^t \\
\text{Var}(\hat{Y}_R) &= \sigma^2 (XA) (X^t X)(XA)^t \\
\end{align*}
$$



## Question 4

```{r}
ridge_reg = function(X, Y, lambda) {
  # X := Design matrix
  # Y := Response variable
  # lambda := Penalty term 
  
  B_ridge = function(X, Y, lambda, transpose = TRUE) {
    out = solve( t(X) %*% X + lambda * diag(ncol(X)) ) %*% (t(X) %*% Y)
    return(out)
  }
  out = sapply(lambda, function(i) {B_ridge(X,Y,i)})
  out = t(out)
  out = as.data.frame(out)
  out$lambda = lambda
  return(out)
}
```

The above function is designed to output the coefficients for an arbitrary ridge regression as long as it given a design matrix, a response variable and a vector containing penalty terms. We use this `ridge_reg` function with the `cars` dataset to generate the following model:

$$ \text{distance} = \beta_{\text{speed}} x_{\text{speed}} + \beta_{\text{speed}^2} x^2_{\text{speed}} + \varepsilon $$

```{r}
lambda = seq(1, 0, length.out = 100)
est_coef = ridge_reg(cbind(as.vector(scale(cars$speed)), as.vector(scale(cars$speed^2))),cars$dist,lambda)
est_coef %>% ggplot(aes(x = lambda, y = V2)) + geom_line() + ylab("Estimated Coefficient of Speed^2") + xlab(expression(paste(lambda))) + ggtitle("Relationship between Coefficient and Lambda")
```

We can see from the chart above that as the value of $\lambda$ increases the coefficient of the quadratic term ($\beta_2$) decreases. This makes perfect sense, as the purpose of introducing $\lambda$ into the least squares estimator is to shrink regression coefficients. 


[1]: http://www.statsci.org/data/general/motorins.html    "Motorins"
[2]: Burnham & Anderson, 2002; Cohen et al., 2003         "AIC_Multicolinearity"